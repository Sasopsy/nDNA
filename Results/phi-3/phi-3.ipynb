{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3127379,"sourceType":"datasetVersion","datasetId":1906303}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.optim import AdamW\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Tuple\nimport json\n\nclass LegalContrastiveDataset(Dataset):\n    def __init__(self, data_path: str, tokenizer, max_length: int = 512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.data = self.load_and_prepare_data(data_path)\n    \n    def load_and_prepare_data(self, data_path: str):\n        # Load your legal clause dataset\n        df = pd.read_csv(data_path)\n        print(f\"Dataset columns: {df.columns.tolist()}\")\n        print(f\"Dataset shape: {df.shape}\")\n        \n        # Your dataset has: clause_text, clause_type, totalwords, totalletters\n        text_col = 'clause_text'\n        category_col = 'clause_type'\n        \n        print(f\"Using text column: '{text_col}'\")\n        print(f\"Using category column: '{category_col}'\")\n        print(f\"Unique clause types: {df[category_col].value_counts()}\")\n        \n        # Create contrastive pairs\n        data = []\n        df_clean = df.dropna(subset=[text_col])  # Remove rows with missing text\n        \n        # Filter out very short clauses (less than 30 characters)\n        df_clean = df_clean[df_clean['totalletters'] >= 30]\n        \n        print(f\"After filtering: {len(df_clean)} clauses\")\n        \n        for idx, row in df_clean.iterrows():\n            clause = str(row[text_col]).strip()\n            clause_type = str(row[category_col]).strip()\n            \n            # Create different question types based on clause type\n            if clause_type.lower() == 'investments':\n                questions = [\n                    f\"What are the investment restrictions in: {clause[:60]}...\",\n                    f\"Explain the investment clause: {clause[:60]}...\",\n                ]\n            elif clause_type.lower() == 'interest':\n                questions = [\n                    f\"What does this interest clause specify: {clause[:60]}...\",\n                    f\"Explain the interest terms: {clause[:60]}...\",\n                ]\n            else:\n                questions = [\n                    f\"What does this {clause_type.lower()} clause mean: {clause[:60]}...\",\n                    f\"Explain this legal clause: {clause[:60]}...\",\n                ]\n            \n            # Add general questions\n            questions.extend([\n                f\"Interpret this legal text: {clause[:60]}...\",\n                f\"What are the key legal points in: {clause[:60]}...\"\n            ])\n            \n            # Use first 2 questions to avoid too much data\n            for question in questions[:2]:\n                data.append({\n                    'anchor': question,\n                    'positive': clause,\n                    'negative': self.get_negative_sample(df_clean, idx, clause_type),\n                    'label': 1\n                })\n        \n        print(f\"Created {len(data)} training examples\")\n        return data\n    \n    def get_negative_sample(self, df, current_idx, current_category):\n        # Get a clause from different category as negative sample\n        different_category = df[df['clause_type'] != current_category]\n        if len(different_category) > 0:\n            return str(different_category.sample(1)['clause_text'].iloc[0]).strip()\n        \n        # Fallback: random sampling excluding current row\n        other_rows = df[df.index != current_idx]\n        if len(other_rows) > 0:\n            return str(other_rows.sample(1)['clause_text'].iloc[0]).strip()\n        else:\n            return \"This is a sample negative legal clause for comparison.\"\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Tokenize anchor, positive, and negative\n        anchor = self.tokenizer(\n            item['anchor'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        positive = self.tokenizer(\n            item['positive'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        negative = self.tokenizer(\n            item['negative'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'anchor_input_ids': anchor['input_ids'].squeeze(),\n            'anchor_attention_mask': anchor['attention_mask'].squeeze(),\n            'positive_input_ids': positive['input_ids'].squeeze(),\n            'positive_attention_mask': positive['attention_mask'].squeeze(),\n            'negative_input_ids': negative['input_ids'].squeeze(),\n            'negative_attention_mask': negative['attention_mask'].squeeze(),\n        }\n\nclass LegalContrastiveModel(nn.Module):\n    def __init__(self, model_name: str = \"microsoft/Phi-3-mini-4k-instruct\"):\n        super().__init__()\n        self.base_model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            trust_remote_code=True\n        )\n        self.hidden_size = self.base_model.config.hidden_size\n        self.projection = nn.Linear(self.hidden_size, 256)  # Project to smaller dim\n        \n    def get_embeddings(self, input_ids, attention_mask):\n        with torch.no_grad():\n            outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n            # Use mean pooling of last hidden states\n            embeddings = outputs.last_hidden_state.mean(dim=1)\n        return self.projection(embeddings)\n    \n    def forward(self, anchor_ids, anchor_mask, pos_ids, pos_mask, neg_ids, neg_mask):\n        anchor_emb = self.get_embeddings(anchor_ids, anchor_mask)\n        positive_emb = self.get_embeddings(pos_ids, pos_mask)\n        negative_emb = self.get_embeddings(neg_ids, neg_mask)\n        \n        return anchor_emb, positive_emb, negative_emb\n\ndef contrastive_loss(anchor, positive, negative, margin=1.0, temperature=0.07):\n    # Cosine similarity\n    pos_sim = F.cosine_similarity(anchor, positive, dim=1)\n    neg_sim = F.cosine_similarity(anchor, negative, dim=1)\n    \n    # InfoNCE-style loss\n    pos_exp = torch.exp(pos_sim / temperature)\n    neg_exp = torch.exp(neg_sim / temperature)\n    \n    loss = -torch.log(pos_exp / (pos_exp + neg_exp))\n    return loss.mean()\n\ndef train_model(data_path: str, epochs: int = 3, batch_size: int = 4, lr: float = 5e-5):\n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    model = LegalContrastiveModel()\n    \n    # Prepare dataset\n    dataset = LegalContrastiveDataset(data_path, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Optimizer\n    optimizer = AdamW(model.projection.parameters(), lr=lr)  # Only train projection layer\n    \n    model.train()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    print(f\"Training on {device}\")\n    print(f\"Dataset size: {len(dataset)}\")\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_idx, batch in enumerate(dataloader):\n            # Move to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward pass\n            anchor_emb, pos_emb, neg_emb = model(\n                batch['anchor_input_ids'], batch['anchor_attention_mask'],\n                batch['positive_input_ids'], batch['positive_attention_mask'],\n                batch['negative_input_ids'], batch['negative_attention_mask']\n            )\n            \n            # Calculate loss\n            loss = contrastive_loss(anchor_emb, pos_emb, neg_emb)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n        \n        avg_loss = total_loss / len(dataloader)\n        print(f'Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}')\n    \n    # Save model\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'tokenizer': tokenizer\n    }, 'legal_contrastive_model.pth')\n    \n    return model, tokenizer\n\ndef inference(model, tokenizer, query: str, legal_clauses: List[str]):\n    \"\"\"Find most relevant legal clause for a query\"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n    \n    # Encode query\n    query_tokens = tokenizer(\n        query,\n        max_length=512,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    ).to(device)\n    \n    with torch.no_grad():\n        query_emb = model.get_embeddings(\n            query_tokens['input_ids'],\n            query_tokens['attention_mask']\n        )\n    \n    # Find best matching clause\n    best_score = -1\n    best_clause = \"\"\n    \n    for clause in legal_clauses:\n        clause_tokens = tokenizer(\n            clause,\n            max_length=512,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        ).to(device)\n        \n        with torch.no_grad():\n            clause_emb = model.get_embeddings(\n                clause_tokens['input_ids'],\n                clause_tokens['attention_mask']\n            )\n            \n            score = F.cosine_similarity(query_emb, clause_emb, dim=1).item()\n            \n            if score > best_score:\n                best_score = score\n                best_clause = clause\n    \n    return best_clause, best_score\n\n# Usage example\nif __name__ == \"__main__\":\n    # Train the model\n    model, tokenizer = train_model(\"/kaggle/input/contracts-clauses-datasets/legal_docs.csv\")\n    \n    # Example inference\n    query = \"What are the termination clauses in employment contracts?\"\n    sample_clauses = [\n        \"Employee may terminate employment with 30 days notice...\",\n        \"Company reserves right to terminate for cause...\",\n        \"Confidentiality obligations survive termination...\"\n    ]\n    \n    best_clause, score = inference(model, tokenizer, query, sample_clauses)\n    print(f\"Best matching clause (score: {score:.3f}): {best_clause}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T21:44:09.778891Z","iopub.execute_input":"2025-09-12T21:44:09.779722Z","iopub.status.idle":"2025-09-12T21:46:29.463728Z","shell.execute_reply.started":"2025-09-12T21:44:09.779687Z","shell.execute_reply":"2025-09-12T21:46:29.462751Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3980d4c911f24ed7bc4a86e7a9c55950"}},"metadata":{}},{"name":"stdout","text":"Dataset columns: ['Unnamed: 0', 'clause_text', 'clause_type', 'totalwords', 'totalletters']\nDataset shape: (21187, 5)\nUsing text column: 'clause_text'\nUsing category column: 'clause_type'\nUnique clause types: clause_type\ninterest                  1010\nbase-salary               1010\nownership_of_shares       1000\npayment                   1000\ntaxes                     1000\ninvestment-company-act    1000\ncompensation              1000\ninvestments               1000\ncapitalization             930\nloans                      920\nDefinitions                890\nHeadings                   860\nWHEREAS                    730\nEntire                     670\nAssignment                 630\nCounterparts               630\nRepresentations            610\nTermination                590\nSeverability               580\nNOW                        540\nMiscellaneous              530\nInsurance                  470\nIndemnification            370\ndividends                  360\nConfidentiality            360\nGoverning                  330\nNotices                    320\nboard                      250\nesop                       223\npayment_terms              220\ngrant                      220\nfinancing                  170\nvesting                    170\nemployee_benefits          150\nconversion_of_shares       120\ninvestment_company         100\ngrant_of_option             90\nshares                      40\nstock_option                30\nforeign_investors           30\nseed                        24\nprivate_equity              10\nName: count, dtype: int64\nAfter filtering: 20944 clauses\nCreated 41888 training examples\nTraining on cuda\nDataset size: 41888\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3530160991.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/contracts-clauses-datasets/legal_docs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;31m# Example inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3530160991.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(data_path, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             anchor_emb, pos_emb, neg_emb = model(\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anchor_input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anchor_attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive_input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive_attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3530160991.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, anchor_ids, anchor_mask, pos_ids, pos_mask, neg_ids, neg_mask)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0manchor_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mpositive_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mnegative_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3530160991.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# Use mean pooling of last hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'"],"ename":"AttributeError","evalue":"'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'","output_type":"error"}],"execution_count":2}]}